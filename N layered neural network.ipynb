{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-layered neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.array(z)\n",
    "    sig = 1/(1+np.exp(-(z)))\n",
    "    return sig,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    z = np.array(z)\n",
    "    m = np.maximum(0,z)\n",
    "    return m,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(da,cache):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(da,cache):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims):\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    np.random.seed(3)\n",
    "    for l in range(1,L):\n",
    "        parameters['w'+str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros(layers_dims[l])\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linerar_forward(a,w,b):\n",
    "    z = np.dot(z,w)+b\n",
    "    cache = (a,w,b)\n",
    "    return z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(a_prev,w,b,activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        z,linear_cache = linear_forward(a_prev,w,b)\n",
    "        a,activation_cache = sigmoid(z)\n",
    "    elif activation == \"relu\":\n",
    "        z,linear_cache = linear_forward(a_prev,w,b)\n",
    "        a,activation_cache = relu(z)\n",
    "        \n",
    "    cache = (linear_cache,activation_cache)\n",
    "    \n",
    "    return a,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(x,parameters):\n",
    "    caches = []\n",
    "    L = 3\n",
    "    a = x\n",
    "    for l in range(1,L):\n",
    "        a_prev = a\n",
    "        a,cache = linear_forward_activation(a_prev,parameters['w'+str(l)],parameters['b'+str(l)],activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    al,cache = linear_forward_activation(a_prev,parameters['w'+str(L)],parameters['b'+str(L)],activation = \"sigmoid\")\n",
    "    \n",
    "    return al,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(al,y):\n",
    "    m = y.shape[1]\n",
    "    cost = (-1/m)*(np.dot(y,np.log(al).T)+np.dot(1-y,np.log(1-al).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dz,cache):\n",
    "    a_prev,w,b = cache\n",
    "    m = a_prev.shape[1]\n",
    "    \n",
    "    dw = (1/m)*np.dot(dz,a_prev.T)\n",
    "    db = (1/m)*np.sum(dz,axis=1,keepdims = True)\n",
    "    \n",
    "    da_prev = np.dot(w.T,dz)\n",
    "    \n",
    "    return da_prev,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(da,cache,activation):\n",
    "    linear_cache,activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dz = relu_backward(da,activation_cache)\n",
    "        da_prev,dw,db = linear_backward(dz,linear_cache)\n",
    "    if activation == \"sigmoid\":\n",
    "        dz = sigmoid_backward(da,activation_cache)\n",
    "        da_prev,dw,db = linear_backward(dz,linear_cache)\n",
    "        \n",
    "    return a_prev,dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propgation(al,y,cache):\n",
    "    \n",
    "    grads = {}\n",
    "    \n",
    "    L = len(cache)\n",
    "    m = al.shape[1]\n",
    "    y = y.reshape(al.shape)\n",
    "    \n",
    "    dal = -(np.divide(y,al)-np.divide(1-y,1-al))\n",
    "    \n",
    "    current_cache = cache[L-1]\n",
    "    \n",
    "    grads[\"da\"+str(L-1)],grads[\"dw\"+str(L)],grads[\"db\"+str(L)] = linear_activation_backward(dal,current_cache,activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        current_cache = cache[l]\n",
    "        \n",
    "        da_prev_temp,dw_temp,db_temp = linear_activation_backward(dal,curent_cache,activation = \"relu\")\n",
    "        \n",
    "        grads[\"da\"+str(l)] = da_prev_temp\n",
    "        grads[\"dw\"+str(l+1)] = dw_temp\n",
    "        grads[\"db\"+str(l+1)] = db_temp\n",
    "        \n",
    "        \n",
    "    return grads    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
